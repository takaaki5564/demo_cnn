{
 "metadata": {
  "name": "",
  "signature": "sha256:869b721e5a23baea9c682e6936453d7d5dcf25fbeae7042728a69256e30b73ac"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from numpy import hstack, vsplit, reshape\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from collections import defaultdict\n",
      "import six\n",
      "import sys\n",
      "import chainer\n",
      "import chainer.link as L\n",
      "from chainer import optimizers, cuda, serializers\n",
      "import chainer.functions as F\n",
      "import argparse\n",
      "from gensim import corpora, matutils\n",
      "import MeCab\n",
      "from matplotlib import pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "\u5358\u7d14\u306aNN\u3067\u30c6\u30ad\u30b9\u30c8\u5206\u985e\n",
      "\u96a0\u308c\u5c64\u306f\uff12\u3064\n",
      "\u6587\u66f8\u30d9\u30af\u30c8\u30eb\u306b\u306fBow\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\n",
      "\"\"\"\n",
      "class InitData():\n",
      "    \n",
      "    def __init__(self):\n",
      "        source = []\n",
      "        target = []\n",
      "        comment = []\n",
      "        word = []\n",
      "\n",
      "    def to_words(self, sentence):\n",
      "    # \u5165\u529b\uff1a'\u3059\u3079\u3066\u81ea\u5206\u306e\u307b\u3046\u3078'\n",
      "    # \u51fa\u529b\uff1atuple(['\u3059\u3079\u3066','\u81ea\u5206','\u306e','\u307b\u3046','\u3078']) tuple=\u7d44\n",
      "        tagger = MeCab.Tagger('mecabrc') #\u5225\u306eTagger\u3092\u4f7f\u3063\u3066\u3082\u3044\u3044\n",
      "        mecab_result = tagger.parse(sentence)\n",
      "        info_of_word = mecab_result.split('\\n')\n",
      "        words = []\n",
      "        for info in info_of_words:\n",
      "            # mecab\u3067\u5206\u3051\u308b\u3068\u6587\u306e\u6700\u5f8c\u306b''\u304c\u3001\u305d\u306e\u624b\u524d\u306b'EOS'\u304c\u6765\u308b\n",
      "            if info == 'EOS' or info == '':\n",
      "                break\n",
      "                #info => '\u306a\\t\u52a9\u8a5e,\u7d42\u52a9\u8a5e,*,*,*,*,\u306a,\u30ca,\u30ca'\n",
      "                #\u5206\u985e\u304c\u4f55\u6bb5\u968e\u304b\u3042\u3063\u3066\u5206\u985e\u306a\u3057\u306a\u3089*\n",
      "                info_elems = info.split(',')\n",
      "                # 6\u756a\u76ee\u306b\u7121\u6d3b\u7528\u5f62\u306e\u5358\u8a9e\u304c\u5165\u308b\n",
      "                # \u3082\u30576\u756a\u76ee\u304c'*'\u3060\u3063\u305f\u30890\u756a\u76ee\u3092\u5165\u308c\u308b\n",
      "                if info_elem[6] == '*':\n",
      "                    # info_elem[0]=> '\u30f4\u30a1\u30f3\u30ed\u30c3\u30b5\u30e0\\t\u540d\u8a5e'\n",
      "                    words.append(info_elem[0][:3])\n",
      "                    continue\n",
      "                words.append(info_elem[6])\n",
      "            return tuple(words)\n",
      "        \n",
      "        def load_data(self, fname):\n",
      "            source = []\n",
      "            targe = []\n",
      "            comment = []\n",
      "            word = []\n",
      "            f = open(fname, \"r\")\n",
      "            \n",
      "            document_list = [] # \u5404\u884c\u306b\u4e00\u6587\u66f8\u3001\u6587\u66f8\u5185\u306e\u8981\u7d20\u306f\u5358\u8a9e\n",
      "            for l in f.readlines():\n",
      "                sample = l.strip().split(\" \", 1) #\u30e9\u30d9\u30eb\u3068\u5358\u8a9e\u306b\u5206\u96e2\n",
      "                print(sample)\n",
      "                label = int(sample[0]) #\u30e9\u30d9\u30eb\n",
      "                target.append(label)\n",
      "                # document_list.append(sample[1].split())\n",
      "                document_list.append(self.to_words(sample[1]))\n",
      "                comment.append(sample[1])\n",
      "                word.append(self.to_words(sample[1]))\n",
      "                print(sample[1])\n",
      "            \n",
      "            print(document_list)\n",
      "            \n",
      "            # \u5358\u8a9e\u8f9e\u66f8\u3092\u4f5c\u6210\n",
      "            dictionary = {}\n",
      "            dictionary = corpora.Dictionary(document_list)\n",
      "            # \u4f4e\u983b\u5ea6\u3068\u9ad8\u983b\u5ea6\u306e\u30ef\u30fc\u30c9\u306f\u9664\u304f\n",
      "            dictionary.filter_extremes(no_below=0, no_above=100)\n",
      "            # no_below: \u6587\u66f8\u304cno_below\u500b\u4ee5\u4e0b\u306e\u5358\u8a9e\u3092\u7121\u8996\n",
      "            # no_above: \u6587\u7ae0\u306e\u5272\u5408\u304cno_above\u4ee5\u4e0a\u306e\u5834\u5408\u7121\u8996\n",
      "            \n",
      "            for document in document_list:\n",
      "                tmp = dictionary.doc2bow(document) #BoW\u8868\u73fe\n",
      "                vec = list(matutils.corpus2dense(\n",
      "                        [tmp],num_terms=len(dictionary)).T[0])\n",
      "                source.append(vec)\n",
      "            \n",
      "            dataset = {}\n",
      "            dataset['source'] = np.array(source)\n",
      "            dataset['target'] = np.array(target)\n",
      "            dataset['comment'] = comment;\n",
      "            dataset['word'] = word;\n",
      "            print(\"------------\")\n",
      "            print(dataset['comment'])\n",
      "            print(\"\u5168\u30c7\u30fc\u30bf\u6570:\", len(dataset['source']))\n",
      "            print(\"\u8f9e\u66f8\u306b\u767b\u9332\u3055\u308c\u305f\u5358\u8a9e\u6570:\", len(dictionary.items()))\n",
      "            \n",
      "            return dataset, dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == '__main__':\n",
      "    # \u5b66\u7fd2\u30d5\u30e9\u30d5\u7528\n",
      "    losses = []\n",
      "    accuracies = []\n",
      "    \n",
      "    initdata = InitData()\n",
      "    \n",
      "    # \u5f15\u6570\u306e\u8a2d\u5b9a parse=\u89e3\u5256\u3059\u308b\n",
      "    parser = argparse.ArgumentParser() # parser\u3092\u4f5c\u6210\n",
      "    parser.add_argument('--gpu  '    , dest='gpu'        , type=int, default=0,            help='1: use gpu, 0: use cpu')\n",
      "    parser.add_argument('--data '    , dest='data'       , type=str, default='input.dat',  help='an input data file')\n",
      "    parser.add_argument('--epoch'    , dest='epoch'      , type=int, default=100,          help='number of epochs to learn')\n",
      "    parser.add_argument('--batchsize', dest='batchsize'  , type=int, default=1,           help='learning minibatch size')\n",
      "    parser.add_argument('--units'    , dest='units'      , type=int, default=100,           help='number of hidden unit')\n",
      "    \n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    batchsize = args.batchsize #minibatch size\n",
      "    n_epoch = args.epoch #epoch\u6570\uff1d\u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\u6570\n",
      "    \n",
      "    # prepare dataset\n",
      "    dataset, dictionary = initdata.load_data(args.data)\n",
      "    \n",
      "    dataset['source']=dataset['source'].astype(np.float32) #\u6587\u66f8\u30d9\u30af\u30c8\u30eb\n",
      "    dataset['target']=dataset['target'].astype(np.int32) #\u30e9\u30d9\u30eb\n",
      "    \n",
      "    x_train, x_test, y_train, y_test, c_train, c_test, word_train, word_test =\\\n",
      "        train_test_split(dataset['source'], dataset['target'], dataset['comment'], dataset['word'], test_size=0.001)\n",
      "    \n",
      "    print(\"---------------\")\n",
      "    \n",
      "    N_test = y_test.size # test data size\n",
      "    N = len(x_train)     # train data size\n",
      "    in_units = x_train.shape[1]  #\u5165\u529b\u5c64\u30e6\u30cb\u30c3\u30c8\u6570\uff08\u8a9e\u5f59\u6570\uff09\n",
      "    print(\"\u5b66\u7fd2\u30c7\u30fc\u30bf\u6570\uff1a\", N)\n",
      "    \n",
      "    n_units = args.units\n",
      "    n_label = 2\n",
      "    \n",
      "    model = chainer.Chain(l1=L.Linear(in_units, n_units),\n",
      "                          l2=L.Linear(n_units, n_units),\n",
      "                          l3=L.Linear(n_units, n_label))\n",
      "    \n",
      "    xp = np\n",
      "    \n",
      "    batchsize = args.batchsize\n",
      "    n_epoch = args.epoch\n",
      "    \n",
      "    def forward(x, t, train=True):\n",
      "        h1 = F.relu(model.l1(x))\n",
      "        h2 = F.relu(model.l2(h1))\n",
      "        y = model.l3(h2)\n",
      "        \n",
      "        return F.softmax_cross_entropy(y,t), F.accuracy(y,t), y.data\n",
      "    \n",
      "    #Setup optimizer\n",
      "    optimizer = optimzers.Adam()\n",
      "    optimzer.setup(model)\n",
      "    \n",
      "    #Learning loop\n",
      "    for epoch in six.moves.range(1, n_epoch+1):\n",
      "        \n",
      "        print('epoch', epoch)\n",
      "        \n",
      "        #training\n",
      "        perm = np.random.permutation(N) #\u30e9\u30f3\u30c0\u30e0\u306a\u6574\u6570\u5217\u30ea\u30b9\u30c8\u3092\u53d6\u5f97\n",
      "        sum_train_loss = 0.0\n",
      "        sum_train_accuracy = 0.0\n",
      "        for i in six.moves.ranage(0, N, batchsize):\n",
      "            \n",
      "            #perm\u3092\u4f7f\u3044x_train,y_train\u304b\u3089\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u9078\u629e\n",
      "            #\u6bce\u56de\u5bfe\u8c61\u3068\u306a\u308b\u30c7\u30fc\u30bf\u306f\u7570\u306a\u308b\n",
      "            x = chainer.Variable(xp.asarray(x_train[perm[i:i+batchsize]])) #source\n",
      "            t = chainer.Variable(xp.asarray(y_train[perm[i:i+batchsize]])) #target\n",
      "            c = chainer.Variable(xp.asarray(c_train[perm[i:i+batchsize]])) #target\n",
      "            w = chainer.Variable(xp.asarray(word_train[perm[i:i+batchsize]])) #target\n",
      "            \n",
      "            print(\"x_train\", x_train)\n",
      "            print(\"x_train\", np.vsplit(x_train, 2))\n",
      "            \n",
      "            model.zerograds()             #\u52fe\u914d\u3092\u30bc\u30ed\u521d\u671f\u5316\n",
      "            loss, acc, y = forward(x, t)   #\u9806\u4f1d\u642c\n",
      "            \n",
      "            sum_train_loss = float(cuda.to_cpu(loss.data))\n",
      "            sum_train_accuracy = float(cuda.to_cpu(acc.data))\n",
      "            \n",
      "            loss.backward()           #\u8aa4\u5dee\u9006\u4f1d\u642c\n",
      "            optimizer.update()         #\u6700\u9069\u5316\uff1a\u91cd\u307f\u3001\u30d0\u30a4\u30a2\u30b9\u306e\u66f4\u65b0\n",
      "            losses.append(loss.data)    #\u8aa4\u5dee\u95a2\u6570\u30b0\u30e9\u30d5\u7528\n",
      "        \n",
      "        #\u8aa4\u5dee\n",
      "        print('train mean loss={}, accuracy={}'.format(sum_train_loss, sum_train_accuracy))\n",
      "        print(np.argmax(y), y, c.data, w.data) #\u51fa\u529b\u7d50\u679c\u4e00\u89a7\n",
      "        \n",
      "        \n",
      "        # evaluation----------------------------\n",
      "        sum_test_loss  = 0.0\n",
      "        sum_test_accuracy = 0.0\n",
      "        for i in six.moves.range(0, N_test, batchsize):\n",
      "            \n",
      "            #all test data\n",
      "            x = chainer.Variable(xp.asarray(x_test[i:i+batchsize]))\n",
      "            t = chainer.Variable(xp.asarray(y_test[i:i+batchsize]))\n",
      "            c = chainer.Variable(xp.asarray(c_test[i:i+batchsize]))\n",
      "            w = chainer.Variable(xp.asarray(word_test[i:i+batchsize]))\n",
      "            \n",
      "            loss, acc, y = forward(x,t,train=False)\n",
      "            \n",
      "            sum_test_loss = float(cuda.to_cpu(loss.data))\n",
      "            sum_test_accuracy = float(cuda.to_cpu(acc.data))\n",
      "            accuracies.append(acc.data) #\u6c4e\u5316\u6027\u80fd\u30b0\u30e9\u30d5\u7528\n",
      "            \n",
      "        print(' test mean loss={}, accuracy={}'.format(  \\\n",
      "                sum_test_loss, sum_test_accuracy)) #\u8aa4\u5dee\n",
      "        print(n.argmax(y), y, c.data, w.data) #\u51fa\u529b\u7d50\u679c\u4e00\u89a7\n",
      "        print(type(y), type(np.argmax(y)))\n",
      "        print(\"------------------------------------\")\n",
      "    \n",
      "    #model, optimizer\u3092\u4fdd\u5b58\n",
      "    print('save the model')\n",
      "    serializers.save_npz('pn_classifier_ffnn2.model', mode)\n",
      "    print('save the optimizer')\n",
      "    serializers.save_npz('pn_classifier_ffnn2.state', optimizer)\n",
      "    \n",
      "    plt.plot(losses, label=\"sum_train_loss\")\n",
      "    plt.plot(accuracies, label=\"sum_train_accuracy\")\n",
      "    ply.yscale('log')\n",
      "    plt.legend()\n",
      "    plt.grid(True)\n",
      "    plt.title(\"loss\")\n",
      "    plt.xlabel(\"epoch\")\n",
      "    plt.ylabel(\"loss\")\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "usage: -c [-h] [--gpu   GPU] [--data  DATA] [--epoch EPOCH]\n",
        "          [--batchsize BATCHSIZE] [--units UNITS]\n",
        "-c: error: unrecognized arguments: -f /home/mluser/.ipython/profile_default/security/kernel-d362c236-b225-4a68-95bd-fe94871de6c6.json --IPKernelApp.parent_appname='ipython-notebook' --profile-dir /home/mluser/.ipython/profile_default --parent=1\n"
       ]
      },
      {
       "ename": "SystemExit",
       "evalue": "2",
       "output_type": "pyerr",
       "traceback": [
        "An exception has occurred, use %tb to see the full traceback.\n",
        "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "To exit: use 'exit', 'quit', or Ctrl-D.\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}