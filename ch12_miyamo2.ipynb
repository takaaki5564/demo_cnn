{
 "metadata": {
  "name": "",
  "signature": "sha256:99defe9f7d6ce83eae5012683a0c70d1d9ff6c2e6d70ce3f8b110bb4a42ae409"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from numpy import hstack, vsplit, reshape\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from collections import defaultdict\n",
      "import six\n",
      "import sys\n",
      "import chainer\n",
      "import chainer.links as L\n",
      "from chainer import optimizers, cuda, serializers\n",
      "import chainer.functions as F\n",
      "import argparse\n",
      "from gensim import corpora, matutils\n",
      "import MeCab\n",
      "from matplotlib import pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class InitData():\n",
      "    def __init__(self):\n",
      "        source = []   # source sentence\n",
      "        target = []   # zatzudan/notzatsudan (0/1)\n",
      "        comment = []  # split sentence\n",
      "        word = []     # words list\n",
      "    \n",
      "    # input: sentence\n",
      "    # output: tuple(split sentence)\n",
      "    def to_words(self, sentence):\n",
      "        tagger = MeCab.Tagger('mecabrc')  # any tagger is ok\n",
      "        mecab_result = tagger.parse(sentence)   # parse by mecab\n",
      "        #print(\"mecab_result:\")\n",
      "        #print(mecab_result)\n",
      "        #print(\"------------\")\n",
      "        \n",
      "        info_of_words = mecab_result.split('\\n')\n",
      "        words = []\n",
      "        \n",
      "        for info in info_of_words:\n",
      "            if info == 'EOS' or info == '': # end of sentence(mecab)\n",
      "                break\n",
      "                \n",
      "            # info => '\u306a\\t\u52a9\u8a5e,\u7d42\u52a9\u8a5e,*,*,*,*,\u306a,\u30ca,\u30ca'\n",
      "            info_elems = info.split(',')\n",
      "            # 6\u756a\u76ee\u306b\u7121\u6d3b\u7528\u7cfb\u306e\u5358\u8a9e\u304c\u5165\u308b\u3002\u3082\u30576\u756a\u76ee\u304c'*'\u3060\u3063\u305f\u30890\u756a\u76ee\u3092\u5165\u308c\u308b\n",
      "            if info_elems[6] == '*':\n",
      "                words.append(info_elems[0][:-3]) #\u8ca0\u306e\u5834\u5408\u3001\u672b\u5c3e\u304b\u3089\u6570\u3048\u308b\n",
      "            else:\n",
      "                words.append(info_elems[6])\n",
      "        #print(words)\n",
      "        return tuple(words)\n",
      "                \n",
      "    # input: filename\n",
      "    # output: \n",
      "    def load_data(self, fname):\n",
      "        source = []\n",
      "        target = []\n",
      "        comment = []\n",
      "        word = []\n",
      "        f = open(fname, \"r\")\n",
      "        \n",
      "        document_list = []\n",
      "        for l in f.readlines():\n",
      "            sample = l.strip().split(\" \", 1)  # label words\n",
      "            #print(\"sample[0]: \", sample[0])\n",
      "            #print(\"sample[1]: \", sample[1])\n",
      "            label = int(sample[0])\n",
      "            target.append(label)\n",
      "            comment.append(sample[1])\n",
      "            words = self.to_words(sample[1])\n",
      "            document_list.append(words) # \u5358\u8a9e\u5206\u5272\u3057\u3066\u6587\u66f8\u30ea\u30b9\u30c8\u306b\u8ffd\u52a0\n",
      "            word.append(words)\n",
      "        #print(\"len(document_list)\", len(document_list))\n",
      "        #print(\"document_list: \", document_list)\n",
      "        \n",
      "        #\u5358\u8a9e\u8f9e\u66f8\u3092\u4f5c\u6210\n",
      "        dictionary = {}\n",
      "        dictionary = corpora.Dictionary(document_list)\n",
      "        dictionary.filter_extremes(no_below=0, no_above=len(document_list))\n",
      "        # no_below: \u4f7f\u308f\u308c\u3066\u3044\u308b\u6587\u66f8\u304cno_below\u500b\u4ee5\u4e0b\u306e\u5358\u8a9e\u3092\u7121\u8996\n",
      "        # no_above: \u4f7f\u308f\u308c\u3066\u308b\u6587\u7ae0\u306e\u5272\u5408\u304cno_above\u4ee5\u4e0a\u306e\u5834\u5408\u7121\u8996\n",
      "        \n",
      "        for document in document_list:\n",
      "            tmp = dictionary.doc2bow(document) #\u6587\u66f8\u3092BoW\u8868\u73fe\n",
      "            vec = list(matutils.corpus2dense([tmp],\\\n",
      "                    num_terms=len(dictionary)).T[0]) #\u8f9e\u66f8\u4e2d\u306e\u5168ID\u3068\u983b\u5ea6\u306e\u30ea\u30b9\u30c8\n",
      "            #print(document,vec)\n",
      "            source.append(vec)\n",
      "        \n",
      "        dataset = {}\n",
      "        dataset['source'] = np.array(source)\n",
      "        dataset['target'] = np.array(target)\n",
      "        dataset['comment'] = comment\n",
      "        dataset['word'] = word\n",
      "        #print(dataset['comment'])\n",
      "        print(\"\u5168\u30c7\u30fc\u30bf\u6570:\", len(dataset['source']))\n",
      "        print(\"\u8f9e\u66f8\u306b\u767b\u9332\u3055\u308c\u305f\u5358\u8a9e\u6570\", len(dictionary.items()))\n",
      "        \n",
      "        return dataset, dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == '__main__':\n",
      "    losses = []\n",
      "    accuracies = []\n",
      "    initdata = InitData()\n",
      "    \n",
      "    dataset, dictionary = initdata.load_data(\"inputall.dat\")\n",
      "    \n",
      "    #\u30c7\u30fc\u30bf\u30bf\u30a4\u30d7\u306e\u5c5e\u6027\u3092\u6307\u5b9a\n",
      "    dataset['source'] = dataset['source'].astype(np.float32) #\u6587\u66f8\u30d9\u30af\u30c8\u30eb\n",
      "    dataset['target'] = dataset['target'].astype(np.int32) #\u30e9\u30d9\u30eb\n",
      "    \n",
      "    x_train, x_test, y_train, y_test, c_train, c_test, word_train, word_test \\\n",
      "        = train_test_split(dataset['source'], dataset['target'], \\\n",
      "            dataset['comment'], dataset['word'], test_size=0.1)\n",
      "    \n",
      "    N_test = y_test.size\n",
      "    N_train = len(x_train)\n",
      "    in_units = x_train.shape[1]\n",
      "    print(\"\u5b66\u7fd2\u30c7\u30fc\u30bf\u6570\uff1a\", N_train)\n",
      "    print(\"\u8a55\u4fa1\u30c7\u30fc\u30bf\u6570\uff1a\", N_test)\n",
      "    \n",
      "    n_units = 100  #\u96a0\u308c\u5c64\u30e6\u30cb\u30c3\u30c8\u6570\n",
      "    n_label = 2   #\u51fa\u529b\u5c64\u30e6\u30cb\u30c3\u30c8\u6570\n",
      "    \n",
      "    #chainer\u30e2\u30c7\u30eb\u5b9a\u7fa9\n",
      "    model = chainer.Chain(l1=L.Linear(in_units, n_units),\n",
      "                          l2=L.Linear(n_units, n_units),\n",
      "                          l3=L.Linear(n_units, n_label))\n",
      "    \n",
      "    batchsize = 1  #\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\n",
      "    n_epoch = 15  #\u5b66\u7fd2\u56de\u6570\n",
      "\n",
      "    #\u9806\u4f1d\u642c\n",
      "    def forward(x, t, train=True):\n",
      "        h1 = F.relu(model.l1(x))\n",
      "        h2 = F.relu(model.l2(h1))\n",
      "        y = model.l3(h2)\n",
      "        \n",
      "        return F.softmax_cross_entropy(y, t), F.accuracy(y, t), y.data\n",
      "    \n",
      "    #\u6700\u9069\u5316\u624b\u6cd5Adam\u306e\u8a2d\u5b9a\n",
      "    optimizer = optimizers.Adam()\n",
      "    optimizer.setup(model)\n",
      "    \n",
      "    #\u5b66\u7fd2\u30eb\u30fc\u30d7\n",
      "    for epoch in six.moves.range(1, n_epoch+1):\n",
      "        #print('epoch', epoch)\n",
      "        \n",
      "        perm = np.random.permutation(N_train) #\u30e9\u30f3\u30c0\u30e0\u306a\u6574\u6570\u5217\u30ea\u30b9\u30c8\u306e\u53d6\u5f97\n",
      "        sum_train_loss = 0.0\n",
      "        sum_train_accuracy = 0.0\n",
      "        \n",
      "        for i in six.moves.range(0, N_train, batchsize):\n",
      "            #perm \u3092\u4f7f\u3044 x_train, y_train\u304b\u3089\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u9078\u629e (\u6bce\u56de\u5bfe\u8c61\u3068\u306a\u308b\u30c7\u30fc\u30bf\u306f\u7570\u306a\u308b)\n",
      "            x = chainer.Variable(np.asarray(x_train[perm[i:i+batchsize]]))\n",
      "            t = chainer.Variable(np.asarray(y_train[perm[i:i+batchsize]]))\n",
      "            c = chainer.Variable(np.asarray(c_train[perm[i:i+batchsize]]))\n",
      "            w = chainer.Variable(np.asarray(word_train[perm[i:i+batchsize]]))\n",
      "            \n",
      "            model.zerograds()           #\u52fe\u914d\u3092\u30bc\u30ed\u521d\u671f\u5316\n",
      "            loss, acc, y = forward(x,t) #\u9806\u4f1d\u642c\n",
      "            \n",
      "            sum_train_loss += float(cuda.to_cpu(loss.data))\n",
      "            sum_train_accuracy += float(cuda.to_cpu(acc.data))\n",
      "            \n",
      "            loss.backward()   #\u8aa4\u5dee\u9006\u4f1d\u642c\n",
      "            optimizer.update()   #\u6700\u9069\u5316\uff1a\u91cd\u307f\u3001\u30d0\u30a4\u30a2\u30b9\u306e\u66f4\u65b0\n",
      "            losses.append(loss.data)  #\u8aa4\u5dee\u95a2\u6570\u30b0\u30e9\u30d5\u7528\n",
      "        \n",
      "        print('epoch={} train mean loss={}, accuracy={}'.\\\n",
      "                  format(epoch, sum_train_loss/N_train, sum_train_accuracy/N_train))\n",
      "        #print(np.argmax(y), y, c.data, w.data)\n",
      "        \n",
      "        \n",
      "    #model\u3068optimizer\u3092\u4fdd\u5b58\n",
      "    print('save the model')\n",
      "    serializers.save_npz('pn_classifier_ffnn2.model', model)\n",
      "    print('save the optimizer')\n",
      "    serializers.save_npz('pn_classifier_ffnn2.state', optimizer)\n",
      "    \n",
      "    #plt.plot(losses, label = \"sum_train_loss\")\n",
      "    #plt.plot(accuracies, label = \"sum_train_accuracy\")\n",
      "    #plt.yscale('log')\n",
      "    #plt.legend()\n",
      "    #plt.grid(True)\n",
      "    #plt.title(\"loss\")\n",
      "    #plt.xlabel(\"epoch\")\n",
      "    #plt.ylabel(\"loss\")\n",
      "    #plt.show()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u5168\u30c7\u30fc\u30bf\u6570: 392\n",
        "\u8f9e\u66f8\u306b\u767b\u9332\u3055\u308c\u305f\u5358\u8a9e\u6570 556\n",
        "\u5b66\u7fd2\u30c7\u30fc\u30bf\u6570\uff1a 352\n",
        "\u8a55\u4fa1\u30c7\u30fc\u30bf\u6570\uff1a 40\n",
        "epoch=1 train mean loss=0.45529664595696057, accuracy=0.78125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch=2 train mean loss=0.12247390731830489, accuracy=0.9545454545454546"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch=3 train mean loss=0.02719686688347296, accuracy=0.9914772727272727"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch=4 train mean loss=0.004265042868527499, accuracy=1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch=5 train mean loss=0.0012942078438672152, accuracy=1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch=6 train mean loss=0.0006570829586549239, accuracy=1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch=7 train mean loss=0.00038907880132848567, accuracy=1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch=8 train mean loss=0.000258233059536327, accuracy=1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch=9 train mean loss=0.00017754191702062434, accuracy=1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch=10 train mean loss=0.00012586401267485186, accuracy=1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch=11 train mean loss=9.206614711067893e-05, accuracy=1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch=12 train mean loss=6.888197226957841e-05, accuracy=1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch=13 train mean loss=5.167248574170199e-05, accuracy=1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch=14 train mean loss=3.953752192583951e-05, accuracy=1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch=15 train mean loss=3.057040951468728e-05, accuracy=1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "save the model\n",
        "save the optimizer\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "        #perm \u3092\u4f7f\u3044 x_train, y_train\u304b\u3089\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u9078\u629e (\u6bce\u56de\u5bfe\u8c61\u3068\u306a\u308b\u30c7\u30fc\u30bf\u306f\u7570\u306a\u308b)\n",
      "        x = chainer.Variable(np.asarray(x_test))\n",
      "        t = chainer.Variable(np.asarray(y_test))\n",
      "        c = chainer.Variable(np.asarray(c_test))\n",
      "        w = chainer.Variable(np.asarray(word_test))\n",
      "\n",
      "        loss, acc, y = forward(x,t)\n",
      "        print(y)\n",
      "\n",
      "        #pred = y#model(x, t).data\n",
      "        #print(\"\u8b58\u5225\u7d50\u679c={},{}\".format(pred, word_test[i]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[  4.73685551  -4.54159498]\n",
        " [  4.6146245   -3.96989179]\n",
        " [  2.38591576  -1.90358591]\n",
        " [  8.16902542  -7.60100269]\n",
        " [  0.39040154   0.40272605]\n",
        " [  9.62938118  -8.98599148]\n",
        " [ -5.28103065   6.15977716]\n",
        " [  1.03153038  -0.41002351]\n",
        " [  7.82531214  -7.34092283]\n",
        " [  6.83348465  -6.32439375]\n",
        " [ -4.97381115   6.09921074]\n",
        " [ -4.13568449   5.58322191]\n",
        " [  2.03499222  -1.61001599]\n",
        " [ -4.91451883   6.15291119]\n",
        " [  5.02312088  -4.69061518]\n",
        " [ -6.24599648   7.48532343]\n",
        " [  5.64534426  -5.22599173]\n",
        " [  8.77373695  -8.24378395]\n",
        " [  1.75593257  -1.31929326]\n",
        " [ -0.48240605   1.22048664]\n",
        " [ -4.80658913   5.99485636]\n",
        " [  9.51828671  -8.96328163]\n",
        " [ -1.12823033   1.71356177]\n",
        " [  6.66440296  -5.93247175]\n",
        " [ -1.68917012   2.57596612]\n",
        " [  6.29856777  -5.16675615]\n",
        " [ -1.09182692   1.82799983]\n",
        " [  6.88884306  -6.27177429]\n",
        " [  7.41782331  -6.93277407]\n",
        " [  4.68739939  -4.37469912]\n",
        " [  4.68739939  -4.37469912]\n",
        " [ -4.97381115   6.09921074]\n",
        " [  0.39040154   0.40272605]\n",
        " [ -1.84619212   2.46005988]\n",
        " [  4.22939634  -3.90653563]\n",
        " [  8.51119614  -7.6335578 ]\n",
        " [ -5.07865763   6.19465446]\n",
        " [ 10.05031204  -9.60162449]\n",
        " [ -4.80658913   5.99485636]\n",
        " [ 11.15728855 -10.47768116]]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}